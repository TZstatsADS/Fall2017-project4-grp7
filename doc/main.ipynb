{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-based Algorithm\n",
    "\n",
    "#### step 1: Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load_data1 import load_data1\n",
    "from load_data2 import load_data2\n",
    "\n",
    "# load ms data:\n",
    "ms_train = pd.read_csv('../data/MS_sample/data_train.csv',usecols=range(1,4))\n",
    "ms_test = pd.read_csv('../data/MS_sample/data_test.csv',usecols=range(1,4))\n",
    "\n",
    "\n",
    "ms_train_df = load_data1(ms_train) # train in pandas dataframe format \n",
    "ms_test_pd =  load_data1(ms_test) # test in pandas dataframe format\n",
    "\n",
    "ms_train_np=ms_train_df.as_matrix()  # train in numpy matrix format \n",
    "ms_test_np=ms_test_pd.as_matrix() # test in numpy matrix format\n",
    "\n",
    "np.save('../output/train1_matrix',ms_train_np)\n",
    "np.save('../output/test1_matrix',ms_test_np)\n",
    "\n",
    "ms_train_df.to_csv('../output/train1_df.csv',header=True,index=True)\n",
    "ms_test_pd.to_csv('../output/test1_df.csv',header=True,index=True)\n",
    "\n",
    "# load each_movie data:\n",
    "movie_train = pd.read_csv('../data/eachmovie_sample/data_train.csv',usecols=[\"Movie\",\"User\",\"Score\"])\n",
    "movie_test = pd.read_csv('../data/eachmovie_sample/data_test.csv',usecols=[\"Movie\",\"User\",\"Score\"])\n",
    "\n",
    "train2,test2=load_data2(movie_train,movie_test)\n",
    "\n",
    "train2_df.to_csv('../output/train2_df.csv',header=True,index=True)\n",
    "test2_df.to_csv('../output/test2_df.csv', header=True, index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2: Similarity Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2.1 Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pearson_correlation import pearsonSimi1\n",
    "from pearson_correlation import pearsonSimi2\n",
    "\n",
    "\n",
    "pearson_correlation1=pearsonSimi1(train1_df)\n",
    "pearson_correlation2=pearsonSimi2(train2_df)\n",
    "\n",
    "pearson_correlation1.to_pickle(\"../output/pearson_correlation1.pkl\")\n",
    "pearson_correlation2.to_pickle(\"../output/pearson_correlation2.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2.2 Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vector_similarity import cosineSimi1\n",
    "from vector_similarity import cosineSimi2\n",
    "\n",
    "cosine_correlation1=cosineSimi1(train1_df)\n",
    "cosine_correlation2=cosineSimi2(train2_df)\n",
    "\n",
    "cosine_correlation1.to_pickle(\"../output/cosine_correlation1.pkl\")\n",
    "cosine_correlation2.to_pickle(\"../output/cosine_correlation2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### step2.3 Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from entropy import entropySimi1\n",
    "from entropy import entropySimi2\n",
    "\n",
    "entropy_correlation1=entropySimi1(train1_df)\n",
    "entropy_correlation2=entropySimi2(train2_df)\n",
    "\n",
    "entropy_correlation1.to_pickle(\"../output/entropy_correlation1.pkl\")\n",
    "entropy_correlation2.to_pickle(\"../output/entropy_correlation2.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### step2.4 Simrank\n",
    "\n",
    "We are assigned data2 for Simrank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from simrank import simrank\n",
    "\n",
    "all_user = np.union1d(train2_df[\"User\"],test2_df[\"User\"])\n",
    "all_movie = np.union1d(train2_data[\"Movie\"],test2_df[\"Movie\"])\n",
    "n_users = len(all_user)\n",
    "n_items = len(all_movie)\n",
    "\n",
    "\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[np.where(all_user==line[2])[0]-1, np.where(all_movie == line[1])[0]-1] = line[3]\n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[np.where(all_user==line[2])[0]-1, np.where(all_movie == line[1])[0]-1] = line[3]\n",
    "\n",
    "simRank_mat = simrank(dense_mat = train_data_matrix, maxIteration = 30, C1 = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### step3 Significance Weighting\n",
    "\n",
    "We choose pearson correlation for significance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from significance_weighting import significance_weighting\n",
    "\n",
    "pearson_devalued_correlation1=significance_weighting(train1_df,pearson_correlation1)\n",
    "pearson_devalued_correlation2=significance_weighting(train2_df,pearson_correlation2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4 Selecting Neighbors (Best-n-estimator)\n",
    "\n",
    "Please note that we combine step 4 to step 6 together in practise because for every selected neighbor we need to do both prediction and evaluation to pick the best number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.1 Neighbors for Pearson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Best_n import selecting_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=20 # number of neighbors\n",
    "pearson_weights_neighbor1, pearson_neighbors1=selecting_neighborhood(n, pearson_correlation1, train1_df)\n",
    "pearson_weights_neighbor2, pearson_neighbors2=selecting_neighborhood(n, pearson_correlation2, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.2 Neighbors for Pearson with Significance Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearsonSig_weights_neighbor1, pearsonSig_neighbors1=selecting_neighborhood(n, pearson_devalued_correlation1, train1_df)\n",
    "pearsonSig_weights_neighbor2, pearsonSig_neighbors2=selecting_neighborhood(n, pearson_devalued_correlation2, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.3 Neighbors for Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_weights_neighbor1,cosine_neighbors1=selecting_neighborhood(n, cosine_correlation1, train1_df)\n",
    "cosine_weights_neighbor2,cosine_neighbors2=selecting_neighborhood(n, cosine_correlation2, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.4 Neighbors for Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_weights_neighbor1,entropy_weights_neighbors1=selecting_neighborhood(n, entropy_correlation1, train1_df)\n",
    "entropy_weights_neighbor2,entropy_weights_neighbors2=selecting_neighborhood(n, entropy_correlation2, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.5 Neighbors for Simrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simRank_weights_neighbor2,simRank_weights_neighbors2=selecting_neighborhood(n, simRank_mat, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5 Rating Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1 Deviation for Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Deviation_for_mean import Deviation_for_mean1\n",
    "from Deviation_for_mean import Deviation_for_mean2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.1 Pearson Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearson_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,pearson_weights_neighbor1, pearson_neighbors1)\n",
    "pearson_predictionByDeviation_2=Deviation_for_mean2(train_df2,test_df2,pearson_weights_neighbor2, pearson_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.2  Pearson with Significance Weighting Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearsonSig_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,pearsonSig_weights_neighbor1, pearsonSig_neighbors1)\n",
    "pearsonSig_predictionByDeviation_2=Deviation_for_mean2(train_df2,test_df2,pearsonSig_weights_neighbor2, pearsonSig_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.3 Vector Similarity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,cosine_weights_neighbor1, cosine_neighbors1)\n",
    "cosine_predictionByDeviation_2=Deviation_for_mean2(train_df2,test_df2,cosine_weights_neighbor2, cosine_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.4 Entropy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,entropy_weights_neighbor1, entropy_neighbors1)\n",
    "entropy_predictionByDeviation_2=Deviation_for_mean2(train_df2,test_df2,entropy_weights_neighbor2, entropy_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.5 Simrank Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simrank_predictionByDeviation_2=Deviation_for_mean2(train_df2,test_df2,simrank_weights_neighbor2, simrank_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2 Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from z_score import z_score1\n",
    "from z_score import z_score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.1 Pearson Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearson_predictionByZscore_1=z_score1(train_df1,test_df1,pearson_weights_neighbor1, pearson_neighbors1)\n",
    "pearson_predictionByZscore_2=z_score2(train_df2,test_df2,pearson_weights_neighbor2, pearson_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.2 Pearson with Significance Weighting Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearsonSig_predictionByZscore_1=z_score1(train_df1,test_df1,pearsonSig_weights_neighbor1, pearsonSig_neighbors1)\n",
    "pearsonSig_predictionByZscore_2=z_score2(train_df2,test_df2,pearsonSig_weights_neighbor2, pearsonSig_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.3 Vector Similarity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_predictionByZscore_1=z_score1(train_df1,test_df1,cosine_weights_neighbor1, cosine_neighbors1)\n",
    "cosine_predictionByZscore_2=z_score2(train_df2,test_df2,cosine_weights_neighbor2, cosine_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.4 Entropy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_predictionByZscore_1=z_score1(train_df1,test_df1,entropy_weights_neighbor1, entropy_neighbors1)\n",
    "entropy_predictionByZscore_2=z_score2(train_df2,test_df2,entropy_weights_neighbor2, entropy_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.5 Simrank Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simrank_predictionByZscore_2=z_score2(train_df2,test_df2,simrank_weights_neighbor2, simrank_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1 Ranked Scoring(for data 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ranked_scoring import ranked_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1.1 Pearson Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_pearson_deviation = ranked_score(test_df1, pearson_predictionByDeviation_1, d = 0.5, alpha = 5)\n",
    "rs_pearson_z = ranked_score(test_df1, pearson_predictionByZscore_1, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1.2 Pearson with Significance Weighting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_pearsonSig_deviation = ranked_score(test_df1,pearsonSig_predictionByDeviation_1, d = 0.5, alpha = 5)\n",
    "rs_pearsonSig_z = ranked_score(test_df1, pearsonSig_predictionByZscore_1, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1.3 Vector Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_cosine_deviation = ranked_score(test_df1,cosine_predictionByDeviation_1, d = 0.5, alpha = 5)\n",
    "rs_cosine_z = ranked_score(test_df1, cosine_predictionByZscore_1, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1.4 Entropy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_entropy_deviation = ranked_score(test_df1,entropy_predictionByDeviation_1, d = 0.5, alpha = 5)\n",
    "rs_entropy_z = ranked_score(test_df1, entropy_predictionByZscore_1, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2 Mean Absolute Error (for data 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mean_absolute_value import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.1 Pearson Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_pearson_deviation = mean_absolute_error(test_df2, pearson_predictionByDeviation_2)\n",
    "mae_pearson_z = mean_absolute_error(test_df2, pearson_predictionByDeviation_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.2 Pearson with Significance Weighting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_pearsonSig_deviation = mean_absolute_error(test_df2, pearsonSig_predictionByDeviation_2)\n",
    "mae_pearsonSig_z = mean_absolute_error(test_df2, pearsonSig_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.3 Vector Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_cosine_deviation = mean_absolute_error(test_df2, cosine_predictionByDeviation_2)\n",
    "mae_cosine_z = mean_absolute_error(test_df2, cosine_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.4 Entropy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_entropy_deviation = mean_absolute_error(test_df2, entropy_predictionByDeviation_2)\n",
    "mae_entropy_z = mean_absolute_error(test_df2, entropy_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.5 Simrank Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_simrank_deviation = mean_absolute_error(test_df2, simrank_predictionByDeviation_2)\n",
    "mae_simrank_z = mean_absolute_error(test_df2, simrank_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3 ROC Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.1 Pearson Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_pearson_deviation = roc_sensitivity(test_df2, pearson_predictionByDeviation_2)\n",
    "roc_pearson_z = roc_sensitivity(test_df2, pearson_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.2 Pearson with Significance Weighting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_pearsonSig_deviation = roc_sensitivity(test_df2, pearsonSig_predictionByDeviation_2)\n",
    "roc_pearsonSig_z = roc_sensitivity(test_df2, pearsonSig_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.3 Vector Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_cosine_deviation = roc_sensitivity(test_df2, cosine_predictionByDeviation_2)\n",
    "roc_cosine_z = roc_sensitivity(test_df2, cosine_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.4 Entropy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_entropy_deviation = roc_sensitivity(test_df2, entropy_predictionByDeviation_2)\n",
    "roc_entropy_z = roc_sensitivity(test_df2, entropy_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.5 Simrank Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_simrank_deviation = roc_sensitivity(test_df2, simrank_predictionByDeviation_2)\n",
    "roc_simrank_z = roc_sensitivity(test_df2, simrank_predictionByZscore_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step7 put step 4 to step 6 together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data1_evluation_comparison import neighbor_size_dfm1\n",
    "from data1_evluation_comparison import neighbor_size_zscore1\n",
    "\n",
    "from data2_evluation_comparison import neighbor_size_dfm_roc2 #dfm+roc\n",
    "from data2_evluation_comparison import neighbor_size_dfm_mae2 #dfm+mae\n",
    "from data2_evluation_comparison import neighbor_size_zscore_roc2 #zscore+roc\n",
    "from data2_evluation_comparison import neighbor_size_dfm_mae2 #zscore+mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################data1, Deviation for Mean#########################\n",
    "score_list_list = []\n",
    "n_opt_list = []\n",
    "for similairty in pearson_correlation1, cosine_correlation1, entropy_correlation1:\n",
    "    score_list, n_opt = neighbor_size_dfm1(train1,test1,similairty)\n",
    "    print(\"similarity done\")\n",
    "    score_list_list.append(score_list)\n",
    "    n_opt_list.append(n_opt)\n",
    "\n",
    "# score here means ranked score\n",
    "score_pearson_dfm = score_list_list[0]\n",
    "score_pearsonSig_dfm = score_list_list[1]\n",
    "score_cosine_dfm = score_list_list[2]\n",
    "score_entropy_dfm = score_list_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################data1, z_score#########################\n",
    "score_list_list_z = []\n",
    "n_opt_list_z = []\n",
    "for similairty in pearson_correlation1, cosine_correlation1, entropy_correlation1:\n",
    "    score_list, n_opt = neighbor_size_zscore1(train1,test1,similairty)\n",
    "    print(\"similarity done\")\n",
    "    score_list_list_z.append(score_list)\n",
    "    n_opt_list_z.append(n_opt)\n",
    "\n",
    "score_pearson_z = score_list_list_z[0]\n",
    "score_pearsonSig_z = score_list_list_z[1]\n",
    "score_cosine_z = score_list_list_z[2]\n",
    "score_entropy_z = score_list_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################dfm+roc######################\n",
    "roc_list, n_opt = neighbor_size_dfm_roc2(train2_df,test2_df,test2_df,pearson_correlation2)\n",
    "\n",
    "roc_list_list = []\n",
    "n_opt_list_roc = []\n",
    "for similairty in pearson_correlation2, pearson_devalued_correlation2,cosine_correlation2, entropy_correlation2,simrank_correlation:\n",
    "    roc_list, n_opt = neighbor_size_dfm2(train2,test_2,test2,similairty)\n",
    "    print(\"similarity done\")\n",
    "    roc_list_list.append(roc_list)\n",
    "    n_opt_list_roc.append(n_opt)\n",
    "    \n",
    "roc_pearson_dfm = roc_list_list2[0]\n",
    "roc_pearsonSig_dfm = roc_list_list2[1]\n",
    "roc_cosine_dfm = roc_list_list2[2]\n",
    "roc_entropy_dfm = roc_list_list2[3]\n",
    "roc_simrank_dfm=roc_list_list2[4]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################dfm+mae######################\n",
    "mae_list, n_opt = neighbor_size_dfm_mae2(train2,test_2,test2,pearson_correlation2)\n",
    "\n",
    "mae_list_list = []\n",
    "n_opt_list_mae = []\n",
    "for similairty in pearson_correlation2, pearson_devalued_correlation2,cosine_correlation2, entropy_correlation2,simrank_correlation:\n",
    "    mae_list, n_opt = neighbor_size_dfm_mae2(train2,test_2,test2,similairty)\n",
    "    print(\"similarity done\")\n",
    "    mae_list_list.append(mae_list)\n",
    "    n_opt_list_mae.append(n_opt)\n",
    "    \n",
    "mae_pearson_dfm = mae_list_list2[0]\n",
    "mae_pearsonSig_dfm = mae_list_list2[1]\n",
    "mae_cosine_dfm = mae_list_list2[2]\n",
    "mae_entropy_dfm = mae_list_list2[3]\n",
    "mae_simrank_dfm=mae_list_list2[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################zscore+roc#######################\n",
    "\n",
    "roc_list2, n_opt = neighbor_size_zscore_roc2(train2,test_2,test2,pearson_correlation2)\n",
    "\n",
    "roc_list_list2 = []\n",
    "n_opt_list_roc2 = []\n",
    "for similairty in pearson_correlation2,pearson_devalued_correlation2, cosine_correlation2, entropy_correlation2,simrank_correlation:\n",
    "    roc_list2, n_opt = neighbor_size_zscore_roc2(train2,test_2,test2,similairty)\n",
    "    print(\"similarity done\")\n",
    "    roc_list_list2.append(roc_list)\n",
    "    n_opt_list_roc2.append(n_opt)\n",
    "    \n",
    "roc_pearson_z = roc_list_list2[0]\n",
    "roc_pearsonSig_z = roc_list_list2[1]\n",
    "roc_cosine_z = roc_list_list2[2]\n",
    "roc_entropy_z = roc_list_list2[3]\n",
    "roc_simrank_z=roc_list_list2[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################zscore+mae######################\n",
    "\n",
    "\n",
    "mae_list, n_opt = neighbor_size_dfm_mae2(train2,test_2,test2,pearson_correlation2)\n",
    "\n",
    "mae_list_list2 = []\n",
    "n_opt_list_mae2 = []\n",
    "for similairty in pearson_correlation2,pearson_devalued_correlation2,cosine_correlation2, entropy_correlation2,simrank_correlation:\n",
    "    mae_list, n_opt = neighbor_size_dfm_mae2(train2,test_2,test2,similairty)\n",
    "    print(\"similarity done\")\n",
    "    mae_list_list2.append(mae_list)\n",
    "    n_opt_list_mae2.append(n_opt)\n",
    "    \n",
    "mae_pearson_z = mae_list_list2[0]\n",
    "mae_pearsonSig_z = mae_list_list2[1]\n",
    "mae_cosine_z = mae_list_list2[2]\n",
    "mae_entropy_z = mae_list_list2[3]\n",
    "mae_simrank_z=mae_list_list2[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model-based Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: load data (MS data is assigned for this algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load_data1 import load_data1\n",
    "\n",
    "ms_train = pd.read_csv('../data/MS_sample/data_train.csv',usecols=range(1,4))\n",
    "ms_test = pd.read_csv('../data/MS_sample/data_test.csv',usecols=range(1,4))\n",
    "\n",
    "\n",
    "ms_train_df = load_data1(ms_train) # train in pandas dataframe \n",
    "ms_test_pd =  load_data1(ms_test) # test in pandas dataframe\n",
    "\n",
    "ms_train_np=ms_train_df.as_matrix()  # train in numpy matrix \n",
    "ms_test_np=ms_test_pd.as_matrix() # test in numpy matrix\n",
    "\n",
    "\n",
    "# save data\n",
    "\n",
    "#np.save('../output/train1_matrix',ms_train_np)\n",
    "#np.save('../output/test1_matrix',ms_test_np)\n",
    "\n",
    "#ms_train_df.to_csv('../output/train1_df.csv',header=True,index=True)\n",
    "#ms_test_pd.to_csv('../output/test1_df.csv',header=True,index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: train mixture model using EM algorithm\n",
    "#### Here we use 3 clusters for example. Later cross-validation is used for choosing the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cluster_model import train_cluster_model\n",
    "from cluster_model import select_stable_models\n",
    "import os\n",
    "\n",
    "X=np.load(\"../output/train1_matrix.npy\")   # train in numpy matrix \n",
    "train_df=pd.read_csv(\"../output/train1_df.csv\",index_col=0) # train in pandas dataframe\n",
    "test_df=pd.read_csv(\"../output/test1_df.csv\",index_col=0) # test in pandas dataframe\n",
    "\n",
    "\"\"\"\n",
    "X: user matrix \n",
    "   each row Xi=[vi1,vi2,...,vin]\n",
    "   Each row is a users and it records every user's votes\n",
    "\n",
    "k: number of clusters\n",
    "\"\"\"\n",
    "\n",
    "# here take k=3 for example:\n",
    "\n",
    "THETA,A,c=train_cluster_model(X,k) # train mixture model\n",
    "\n",
    "# save model\n",
    "filename='cluster_'+str(k)+'_model.npz'\n",
    "base_dir=\"../output\"\n",
    "np.savez(os.path.join(base_dir,filename),THETA=THETA,A=A,c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from part2_prediction_matrix import mixture_model_prediction\n",
    "\n",
    "mixture_model=np.load(\"../output/cluster_3_model.npz\") # load model\n",
    "train_df=pd.read_csv(\"../output/train1_df.csv\",index_col=0)\n",
    "    \n",
    "THETA=mixture_model['THETA']\n",
    "A=mixture_model['A'] #assign matrix\n",
    "prediction_matrix=mixture_model_prediction(A,THETA) # get prediction matrix\n",
    "    \n",
    "# transform numpy matrix to pandas matrix\n",
    "\n",
    "prediction_df=pd.DataFrame(prediction_matrix)\n",
    "prediction_df.index=train_df.index\n",
    "prediction_df.columns=train_df.columns\n",
    "   \n",
    "prediction_df.to_csv(\"../output/train1_prediction_df.csv\",header=True,index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ranked_scoring import ranked_score\n",
    "\n",
    "cluster3_score=ranked_score(test_df,prediction_df)\n",
    "print (cluster3_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 5: choose the number of clusters\n",
    "As shown above, ranked score for k=3 is got. We used this method to get the cluster with highest ranked score.\n",
    "It should be known that select_stable_models is used here. This function will repeat train_cluster function 10 times and select the model with highest log likelihood. It is because sometimes the model is not the optimal model if it is just trained once. But after 10 times it should be a stable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K is a range of number of clusters.\n",
    "# get the ranked_score for cluster 3 - cluster 15\n",
    "    K=range(3,16)\n",
    "    cluster_scores=[]\n",
    "    for k in K:\n",
    "        THETA,A,c,log_likelihood=select_stable_models(X,k)\n",
    "        prediction_df=mixture_model_prediction(A,THETA)\n",
    "        # transform numpy matrix to pandas dataframe\n",
    "        prediction_df=pd.DataFrame(prediction_df)\n",
    "        prediction_df.index=train_df.index\n",
    "        prediction_df.columns=train_df.columns\n",
    "\n",
    "        cluster_score=ranked_score(test_df,prediction_df)\n",
    "        cluster_scores.append(cluster_score)\n",
    "print(cluster_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is :\n",
    "[32.02300583,\n",
    "31.9424132522,\n",
    "31.9444209026,\n",
    "28.5646816105,\n",
    "31.6779248212,\n",
    "29.9055972369,\n",
    "29.0201072183,\n",
    "29.7544491759,\n",
    "27.495674986,\n",
    "31.3537157196,\n",
    "31.8771600791,\n",
    "27.9064300172,\n",
    "31.6641433513,\n",
    "]\n",
    " \n",
    " So we choose k=5 for our model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
