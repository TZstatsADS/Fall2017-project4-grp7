{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-based Algorithm\n",
    "\n",
    "#### step 1: Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load_data1 import load_data1\n",
    "from load_data2 import load_data2\n",
    "\n",
    "# load ms data:\n",
    "ms_train = pd.read_csv('../data/MS_sample/data_train.csv',usecols=range(1,4))\n",
    "ms_test = pd.read_csv('../data/MS_sample/data_test.csv',usecols=range(1,4))\n",
    "\n",
    "\n",
    "ms_train_df = load_data1(ms_train) # train in pandas dataframe format \n",
    "ms_test_pd =  load_data1(ms_test) # test in pandas dataframe format\n",
    "\n",
    "ms_train_np=ms_train_df.as_matrix()  # train in numpy matrix format \n",
    "ms_test_np=ms_test_pd.as_matrix() # test in numpy matrix format\n",
    "\n",
    "np.save('../output/train1_matrix',ms_train_np)\n",
    "np.save('../output/test1_matrix',ms_test_np)\n",
    "\n",
    "ms_train_df.to_csv('../output/train1_df.csv',header=True,index=True)\n",
    "ms_test_pd.to_csv('../output/test1_df.csv',header=True,index=True)\n",
    "\n",
    "# load each_movie data:\n",
    "movie_train = pd.read_csv('../data/eachmovie_sample/data_train.csv',usecols=[\"Movie\",\"User\",\"Score\"])\n",
    "movie_test = pd.read_csv('../data/eachmovie_sample/data_test.csv',usecols=[\"Movie\",\"User\",\"Score\"])\n",
    "\n",
    "train2_df=load_data2(movie_train)\n",
    "test2_df=load_data2(movie_test)\n",
    "\n",
    "train2_df.to_csv('../output/train2_df.csv',header=True,index=True)\n",
    "test2_df.to_csv('../output/test2_df.csv', header=True, index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pearson_correlation import pearsonSimi1\n",
    "from pearson_correlation import pearsonSimi2\n",
    "from vector_similarity import cosineSimi1\n",
    "from vector_similarity import cosineSimi2\n",
    "from simrank import simrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2: Similarity Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2.1 Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pearson_correlation import pearsonSimi1\n",
    "from pearson_correlation import pearsonSimi2\n",
    "\n",
    "\n",
    "pearson_correlation1=pearsonSimi1(train1_df)\n",
    "pearson_correlation2=pearsonSimi2(train2_df)\n",
    "\n",
    "pearson_correlation1.to_pickle(\"../output/pearson_correlation1.pkl\")\n",
    "pearson_correlation2.to_pickle(\"../output/pearson_correlation2.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2.2 Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vector_similarity import cosineSimi1\n",
    "from vector_similarity import cosineSimi2\n",
    "\n",
    "cosine_correlation1=cosineSimi1(train1_df)\n",
    "cosine_correlation2=cosineSimi2(train2_df)\n",
    "\n",
    "cosine_correlation1.to_pickle(\"../output/cosine_correlation1.pkl\")\n",
    "cosine_correlation2.to_pickle(\"../output/cosine_correlation2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### step2.3 Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from entropy import entropySimi1\n",
    "from entropy import entropySimi2\n",
    "\n",
    "entropy_correlation1=entropySimi1(train1_df)\n",
    "entropy_correlation2=entropySimi2(train2_df)\n",
    "\n",
    "entropy_correlation1.to_pickle(\"../output/entropy_correlation1.pkl\")\n",
    "entropy_correlation2.to_pickle(\"../output/entropy_correlation2.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### step2.4 Simrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### step3 Significance Weighting\n",
    "\n",
    "We choose pearson correlation for significance weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from significance_weighting import significance_weighting\n",
    "\n",
    "pearson_devalued_correlation1=significance_weighting(train1_df,pearson_correlation1)\n",
    "pearson_devalued_correlation2=significance_weighting(train2_df,pearson_correlation2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4 Selecting Neighbors (Best-n-estimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Best_n import selecting_neighborhood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.1 Neighbors for Pearson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=20 # number of neighbors\n",
    "pearson_weights_neighbor1, pearson_neighbors1=selecting_neighborhood(n, pearson_correlation1, train1_df)\n",
    "pearson_weights_neighbor2, pearson_neighbors2=selecting_neighborhood(n, pearson_correlation2, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.2 Neighbors for Pearson with Significance Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearsonSig_weights_neighbor1, pearsonSig_neighbors1=selecting_neighborhood(n, pearson_devalued_correlation1, train1_df)\n",
    "pearsonSig_weights_neighbor2, pearsonSig_neighbors2=selecting_neighborhood(n, pearson_devalued_correlation2, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.3 Neighbors for Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_weights_neighbor1=selecting_neighborhood(n, cosine_correlation1, train1_df)\n",
    "cosine_weights_neighbor2=selecting_neighborhood(n, cosine_correlation2, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.4 Neighbors for Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_weights_neighbor1=selecting_neighborhood(n, entropy_correlation1, train1_df)\n",
    "entropy_weights_neighbor2=selecting_neighborhood(n, entropy_correlation2, train2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4.5 Neighbors for Simrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5 Rating Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1 Deviation for Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Deviation_for_mean import Deviation_for_mean1\n",
    "from Deviation_for_mean import Deviation_for_mean2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.1 Pearson Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearson_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,pearson_weights_neighbor1, pearson_neighbors1)\n",
    "pearson_predictionByDeviation_2=Deviation_for_mean1(train_df2,test_df2,pearson_weights_neighbor2, pearson_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.2  Pearson with Significance Weighting Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearsonSig_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,pearsonSig_weights_neighbor1, pearsonSig_neighbors1)\n",
    "pearsonSig_predictionByDeviation_2=Deviation_for_mean1(train_df2,test_df2,pearsonSig_weights_neighbor2, pearsonSig_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.3 Vector Similarity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,cosine_weights_neighbor1, cosine_neighbors1)\n",
    "cosine_predictionByDeviation_2=Deviation_for_mean1(train_df2,test_df2,cosine_weights_neighbor2, cosine_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.4 Entropy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,cosine_weights_neighbor1, cosine_neighbors1)\n",
    "entropy_predictionByDeviation_2=Deviation_for_mean1(train_df2,test_df2,cosine_weights_neighbor2, cosine_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.1.5 Simrank Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_predictionByDeviation_1=Deviation_for_mean1(train_df1,test_df1,cosine_weights_neighbor1, cosine_neighbors1)\n",
    "entropy_predictionByDeviation_2=Deviation_for_mean1(train_df2,test_df2,cosine_weights_neighbor2, cosine_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2 Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from z_score import z_score1\n",
    "from z_score import z_score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.1 Pearson Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearson_predictionByZscore_1=z_score1(train_df1,test_df1,pearson_weights_neighbor1, pearson_neighbors1)\n",
    "pearson_predictionByZscore_2=z_score2(train_df2,test_df2,pearson_weights_neighbor2, pearson_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.2 Pearson with Significance Weighting Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearsonSig_predictionByZscore_1=z_score1(train_df1,test_df1,pearson_weights_neighbor1, pearson_neighbors1)\n",
    "pearsonSig_predictionByZscore_2=z_score2(train_df2,test_df2,pearson_weights_neighbor2, pearson_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.3 Vector Similarity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_predictionByZscore_1=z_score1(train_df1,test_df1,cosine_weights_neighbor1, cosine_neighbors1)\n",
    "cosine_predictionByZscore_2=z_score2(train_df2,test_df2,cosine_weights_neighbor2, cosine_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.4 Entropy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_predictionByZscore_1=z_score1(train_df1,test_df1,entropy_weights_neighbor1, entropy_neighbors1)\n",
    "entropy_predictionByZscore_2=z_score2(train_df2,test_df2,entropy_weights_neighbor2, entropy_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step5.2.5 Simrank Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_predictionByZscore_1=z_score1(train_df1,test_df1,entropy_weights_neighbor1, entropy_neighbors1)\n",
    "entropy_predictionByZscore_2=z_score2(train_df2,test_df2,entropy_weights_neighbor2, entropy_neighbors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1 Ranked Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1.1 Pearson Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_pearson = ranked_score(test_df1, pred_pearson1, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1.2 Pearson with Significance Weighting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_sig_pearson = ranked_score(test_df1, pred_sig_pearson1, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1.3 Vector Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_cosine = ranked_score(test_df1, pred_cosine1, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.1.4 Entropy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs_entropy = ranked_score(test_df1, pred_entropy1, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2 Mean Absolute Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.1 Pearson Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_pearson = mean_absolute_error(test_df2, pred_pearson2, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.2 Pearson with Significance Weighting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_sig_pearson = mean_absolute_error(test_df2, pred_sig_pearson2, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.3 Vector Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_cosine = mean_absolute_error(test_df2, pred_cosine2, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.4 Entropy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_entropy = mean_absolute_error(test_df2, pred_entropy2, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.2.5 Simrank Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mae_simrank = mean_absolute_error(test_df2, pred_simrank, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3 ROC Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.1 Pearson Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_pearson = roc_sensitivity(test_df2, pred_pearson2, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.2 Pearson with Significance Weighting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_sig_pearson = roc_sensitivity(test_df2, pred_sig_pearson2, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.3 Vector Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_cosine = roc_sensitivity(test_df2, pred_cosine2, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.4 Entropy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_entropy = roc_sensitivity(test_df2, pred_entropy2, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step6.3.5 Simrank Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_simrank = roc_sensitivity(test_df2, pred_simrank, d = 0.5, alpha = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model-based Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: load data (MS data is assigned for this algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load_data1 import load_data1\n",
    "\n",
    "ms_train = pd.read_csv('../data/MS_sample/data_train.csv',usecols=range(1,4))\n",
    "ms_test = pd.read_csv('../data/MS_sample/data_test.csv',usecols=range(1,4))\n",
    "\n",
    "\n",
    "ms_train_df = load_data1(ms_train) # train in pandas dataframe \n",
    "ms_test_pd =  load_data1(ms_test) # test in pandas dataframe\n",
    "\n",
    "ms_train_np=ms_train_df.as_matrix()  # train in numpy matrix \n",
    "ms_test_np=ms_test_pd.as_matrix() # test in numpy matrix\n",
    "\n",
    "\n",
    "# save data\n",
    "\n",
    "#np.save('../output/train1_matrix',ms_train_np)\n",
    "#np.save('../output/test1_matrix',ms_test_np)\n",
    "\n",
    "#ms_train_df.to_csv('../output/train1_df.csv',header=True,index=True)\n",
    "#ms_test_pd.to_csv('../output/test1_df.csv',header=True,index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: train mixture model using EM algorithm\n",
    "#### Here we use 3 clusters for example. Later cross-validation is used for choosing the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cluster_model import train_cluster_model\n",
    "from cluster_model import select_stable_models\n",
    "import os\n",
    "\n",
    "X=np.load(\"../output/train1_matrix.npy\")   # train in numpy matrix \n",
    "train_df=pd.read_csv(\"../output/train1_df.csv\",index_col=0) # train in pandas dataframe\n",
    "test_df=pd.read_csv(\"../output/test1_df.csv\",index_col=0) # test in pandas dataframe\n",
    "\n",
    "\"\"\"\n",
    "X: user matrix \n",
    "   each row Xi=[vi1,vi2,...,vin]\n",
    "   Each row is a users and it records every user's votes\n",
    "\n",
    "k: number of clusters\n",
    "\"\"\"\n",
    "\n",
    "# here take k=3 for example:\n",
    "\n",
    "THETA,A,c=train_cluster_model(X,k) # train mixture model\n",
    "\n",
    "# save model\n",
    "filename='cluster_'+str(k)+'_model.npz'\n",
    "base_dir=\"../output\"\n",
    "np.savez(os.path.join(base_dir,filename),THETA=THETA,A=A,c=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 3: prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from part2_prediction_matrix import mixture_model_prediction\n",
    "\n",
    "mixture_model=np.load(\"../output/cluster_3_model.npz\") # load model\n",
    "train_df=pd.read_csv(\"../output/train1_df.csv\",index_col=0)\n",
    "    \n",
    "THETA=mixture_model['THETA']\n",
    "A=mixture_model['A'] #assign matrix\n",
    "prediction_matrix=mixture_model_prediction(A,THETA) # get prediction matrix\n",
    "    \n",
    "# transform numpy matrix to pandas matrix\n",
    "\n",
    "prediction_df=pd.DataFrame(prediction_matrix)\n",
    "prediction_df.index=train_df.index\n",
    "prediction_df.columns=train_df.columns\n",
    "   \n",
    "prediction_df.to_csv(\"../output/train1_prediction_df.csv\",header=True,index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ranked_scoring import ranked_score\n",
    "\n",
    "cluster3_score=ranked_score(test_df,prediction_df)\n",
    "print (cluster3_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 5: choose the number of clusters\n",
    "As shown above, ranked score for k=3 is got. We used this method to get the cluster with highest ranked score.\n",
    "It should be known that select_stable_models is used here. This function will repeat train_cluster function 10 times and select the model with highest log likelihood. It is because sometimes the model is not the optimal model if it is just trained once. But after 10 times it should be a stable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K is a range of number of clusters.\n",
    "# get the ranked_score for cluster 3 - cluster 20\n",
    "    K=range(3,21)\n",
    "    cluster_scores=[]\n",
    "    for k in K:\n",
    "        THETA,A,c,log_likelihood=select_stable_models(X,k)\n",
    "        prediction_df=mixture_model_prediction(A,THETA)\n",
    "        # transform numpy matrix to pandas dataframe\n",
    "        prediction_df=pd.DataFrame(prediction_df)\n",
    "        prediction_df.index=train_df.index\n",
    "        prediction_df.columns=train_df.columns\n",
    "\n",
    "        cluster_score=ranked_score(test_df,prediction_df)\n",
    "        cluster_scores.append(cluster_score)\n",
    "print(cluster_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is :\n",
    "[81.988519767745032,\n",
    " 79.850197156108479,\n",
    " 78.15185123921205,\n",
    " 70.530292271372431,\n",
    " 75.295554154425162,\n",
    " 73.033652276173811,\n",
    " 68.861695574890973,\n",
    " 74.153784419504106,\n",
    " 77.004353981220802,\n",
    " 70.585141069716869,\n",
    " 70.879506091305586,\n",
    " 73.525475762373318,\n",
    " 72.013816579444793,\n",
    " 72.687744999995289,\n",
    " 75.569944504156027,\n",
    " 72.808830626227945,\n",
    " 68.287269638291136,\n",
    " 71.986778485624086]\n",
    " \n",
    " So we choose k=3 for our model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
